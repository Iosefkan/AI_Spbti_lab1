# Отчет по первой лабораторной работе

## 1. Теоретическая база
Цель работы: научиться реализовывать один из алгоритмов глубокого обучения с нуля.

В основе данной работы лежит задача классификации изображений с применением методов глубокого обучения. Современные нейронные сети, обучаемые на больших наборах данных, демонстрируют высокую точность в решении таких задач. 
Идея CNN заключается в том, чтобы применить набор фильтров ко входному изображению, постепенно выявляя как низкоуровневые признаки (например, контуры или грани), так и более сложные структуры, формируемые на последующих слоях. В отличие от классических подходов, где специалистам приходилось вручную выделять особенности изображений, глубокие сети обучаются делать это автоматически.

Сверточные нейронные сети (CNN) — это тип глубоких нейронных сетей, специально разработанных для обработки данных с решетчатой структурой, таких как изображения. CNN применяют свертку — математическую операцию, которая вычисляет корреляцию между маской (ядром) и входными данными. Это позволяет сети обнаруживать паттерны и особенности в данных, независимо от их пространственного положения. CNN широко используются в таких задачах, как распознавание образов, обработка естественного языка и компьютерное зрение.

EfficientNet — это семейство CNN, разработанных для достижения высокой точности при минимальных вычислительных затратах. EfficientNet используют метод масштабируемой глубины и ширины, который позволяет настраивать сеть в соответствии с желаемым уровнем производительности. Сети EfficientNet состоят из нескольких блоков свертки, которые соединены сложными путями. Они добиваются высокой эффективности благодаря использованию составных сверток и squeeze-and-excitation-блоков, которые уменьшают размер каналов и улучшают взаимозависимость между ними.

Nesterov Adam (NAdam) — это алгоритм оптимизации, который является вариантом популярного алгоритма Adam. NAdam добавляет в Adam импульс Nesterov, который улучшает сходимость за счет использования оценок будущих градиентов. Импульс Nesterov учитывает направление предыдущих градиентов, что позволяет сети более эффективно перемещаться по ландшафту потерь. NAdam широко используется в глубоком обучении, так как он позволяет достичь более быстрой и стабильной сходимости.

### Описание оптимизаторов EfficientNet и NAdam (Nesterov Adam)

1. **EfficientNet:**
   EfficientNet - это семейство моделей сверточной нейронной сети (CNN), оптимизированных для достижения высокой точности со сниженными вычислительными затратами. Они были разработаны с помощью метода поиска нейронной архитектуры (NAS), который автоматически ищет оптимальные архитектуры сети.

   **Основные свойства:**
    - Лёгкие модели: EfficientNet-B0 содержит всего 5,3 млн параметров, что в несколько раз меньше, чем у традиционных CNN.
    - Высокая точность: Несмотря на небольшой размер, EfficientNet достигает высокой точности на различных задачах распознавания изображений, включая классификацию, локализацию объектов и сегментацию.
    - Масштабируемость: Архитектура EfficientNet может быть масштабирована для создания более крупных моделей с более высокой точностью, таких как EfficientNet-B7.
    - Обработка изображений произвольного размера: EfficientNet может обрабатывать изображения произвольного размера без потери точности, что делает их подходящими для таких задач, как масштабирование объектов.

2. **NAdam (Nesterov Adam):**
   NAdam - это оптимизатор градиентного спуска, который сочетает в себе алгоритмы Nesterov и Adam. Он был разработан для ускорения процесса обучения и повышения точности моделей.

   **Свойства:**
    - Ускоренное обучение: NAdam использует импульс, похожий на Nesterov, который позволяет модели делать большие шаги в направлении оптимума.
    - Адаптивные скорости обучения: Подобно Adam, NAdam адаптирует скорость обучения для каждого параметра модели, что позволяет быстро учиться для важных параметров и медленно учиться для незначительных параметров.
    - Уменьшение колебаний: NAdam также использует скользящие средние для градиентов и вторых производных градиента, что помогает уменьшить колебания в процессе обучения.
    - Робастность и стабильность: NAdam - робастный и стабильный оптимизатор, который хорошо работает с различными моделями и задачами. 
    - Введение дополнительного параметра сглаживания `alpha_t`, который определяется динамически для каждого шага обучения в зависимости от поведения градиента.

   **Формулы обновления:**
    - `v_t = β₁  v_{t-1} + (1 - β₁)  g_t`:
        Обновление экспоненциально взвешенной средней градиентов, `v_t`.
        Взвешивание текущего градиента `g_t` с предыдущим средним `v_{t-1}` с помощью `β₁`.
    - `m_t = β₂  m_{t-1} + (1 - β₂)  x_t`:
        Обновление экспоненциально взвешенной средней вторых производных градиента, `m_t`.
        Взвешивание текущей второй производной `x_t` с предыдущим средним `m_{t-1}` с помощью `β₂`.
    - `v_t'^ = v_t / (1 - β₁^t)`:
        Исправление смещения экспоненциально взвешенной средней градиентов, чтобы компенсировать стартовое смещение.
    - `m_t'^ = m_t / (1 - β₂^t)`:
        Исправление смещения экспоненциально взвешенной средней вторых производных градиента, чтобы компенсировать стартовое смещение.
    - `x_t+1 = x_t - η  v_t'^  (β₁  m_t' + (1 - β₁)  g_t)`:
        Обновление параметров модели `x_t` в направлении отрицательного градиента.
        Использование взвешенной суммы исправленных скользящих средних `β₁  m_t'` и `(1 - β₁)  g_t` для определения направления обновления.
        Умножение на скорость обучения `η` для контроля размера шага.
    - где:
        `t` - номер текущей итерации
        `g_t` - градиент потери
        `x_t` - текущее значение параметров модели
        `v_t` - экспоненциально взвешенная средняя градиентов
        `m_t` - экспоненциально взвешенная средняя вторых производных градиента
        `β₁` и `β₂` - постоянные скорости обучения
        `η` - скорость обучения

## 2. Разработанная система

**Копмоненты разработанной системы:**
- Сверточные блоки (ConvBlock):
Класс ConvBlock представляет собой базовый строительный блок, включающий сверточный слой, нормализацию по батчу (BatchNorm2d) и нелинейную активацию (SiLU).
Используется для построения глубокой нейронной сети.
- Блоки сжатия и активации (Squeeze-Excitation, SE):
Класс SqueezeExcitation реализует SE-механизм, который увеличивает значимость важных каналов.
Сначала используется среднее значение по пространству (AdaptiveAvgPool2d), а затем два полносвязных слоя с нелинейностью (SiLU и Sigmoid), чтобы взвесить каждый канал.
- Мобильные блоки (MBBlock):
Класс MBBlock представляет собой основу сети, включающую:
Опциональное расширение каналов.
Глубокую сверточную сеть с SE-блоками.
Свертку с уменьшением размерности.
Поддерживает остаточные соединения, если входные и выходные размеры совпадают.
- Сеть EfficientNet:
Класс EfficientNet реализует адаптивную структуру модели, где масштабируются ширина (количество каналов), глубина (количество слоев) и разрешение изображения в соответствии с параметрами версии модели.
Включает:
Базовый блок сверток (ConvBlock) для начальной обработки.
Стек из нескольких MBBlock, параметры которых заданы в basic_mb_params.
Заключительный блок для увеличения числа каналов до фиксированного значения.
Классификатор с адаптивным объединением (AdaptiveAvgPool2d) и полносвязным слоем.
Используется методика масштабирования параметров (Compound Scaling), задаваемая через параметры ϕ, α, β.
- Оптимизаторы (Adam и Nadam):
Adam — стандартный оптимизатор на основе адаптивного момента.
Nadam — кастомная реализация оптимизатора Nadam, включающего инерцию Нестерова.
Оба оптимизатора сравниваются на задаче классификации.
- Датасет (CarModelsDataset):
Загружает аннотации и изображения для классификации автомобилей.
Использует библиотеку Pillow для обработки изображений и поддерживает преобразования через torchvision.transforms.
- Загрузка данных:
Используются DataLoader для организации мини-батчей.
Преобразования изображений включают изменение размера, нормализацию и преобразование в тензоры.
- Тренировочная и тестовая процедуры:
Функция train_test реализует процесс обучения модели:
Оптимизация производится на основе функции потерь CrossEntropyLoss.
Метрики включают потери и точность на обучающих и тестовых данных.
Выводится точность на каждом этапе обучения.

**Принцип работы системы:**
- Система использует датасет Stanford Cars, который состоит из изображений автомобилей и аннотаций, описывающих класс (марку и модель).
- Датасет разбит на тренировочную и тестовую выборки.
- Изображения нормализуются и масштабируются под размер 224x224.
- Инициализируется EfficientNet.
- Запускается процесс обучения: сеть итеративно обрабатывает батчи изображений, вычисляет ошибку (кросс-энтропийная функция потерь) и обновляет веса с помощью выбранного оптимизатора.
- После обучения модель оценивается на тестовой выборке, вычисляется точность классификации.
  
**Алгоритм работы системы**:
1. Загрузка и парсинг аннотаций из .mat файлов.
2. Создание Dataset и DataLoader для удобной итерации по данным.
3. Определение и инициализация модели EfficientNet.
4. Инициализация оптимизаторов Nadam и Adam.
5. Запуск циклов обучения.
6. Вывод анализа результатов после каждой эпохи.


## 3. Результат выполнения программы
```
Обучение с NAdam
Эпоха 1, Время обучения: 71.89c., Потери: 6.6876, Точность: 0.56%
Эпоха 2, Время обучения: 69.89c., Потери: 6.0975, Точность: 0.83%
Эпоха 3, Время обучения: 69.98c., Потери: 5.7257, Точность: 0.93%
Эпоха 4, Время обучения: 76.27c., Потери: 5.5280, Точность: 0.97%
Эпоха 5, Время обучения: 69.74c., Потери: 5.4138, Точность: 1.04%
Эпоха 6, Время обучения: 69.73c., Потери: 5.3363, Точность: 1.03%
Эпоха 7, Время обучения: 69.61c., Потери: 5.2786, Точность: 1.42%
Эпоха 8, Время обучения: 70.11c., Потери: 5.2291, Точность: 1.55%
Эпоха 9, Время обучения: 70.27c., Потери: 5.1882, Точность: 1.62%
Эпоха 10, Время обучения: 69.35c., Потери: 5.1344, Точность: 1.90%
Тестовая точность: 1.31%

-----------------------------------

Обучение с Adam
Эпоха 1, Время обучения: 61.97c., Потери: 6.6935, Точность: 0.63%
Эпоха 2, Время обучения: 62.01c., Потери: 6.1046, Точность: 0.85%
Эпоха 3, Время обучения: 65.55c., Потери: 5.7298, Точность: 0.87%
Эпоха 4, Время обучения: 64.55c., Потери: 5.5326, Точность: 1.01%
Эпоха 5, Время обучения: 64.61c., Потери: 5.4187, Точность: 1.08%
Эпоха 6, Время обучения: 64.67c., Потери: 5.3436, Точность: 1.19%
Эпоха 7, Время обучения: 62.77c., Потери: 5.2857, Точность: 1.42%
Эпоха 8, Время обучения: 62.2c., Потери: 5.2430, Точность: 1.28%
Эпоха 9, Время обучения: 62.11c., Потери: 5.1966, Точность: 1.61%
Эпоха 10, Время обучения: 62.13c., Потери: 5.1430, Точность: 2.05%
Тестовая точность: 1.29%
```

## 4. Использованные источники

Крижевский А. В., Суцкевер И. В., Хинтон Г. Е. Классификация ImageNet с глубокими сверточными нейронными сетями // Коммуникации АКМ. – 2017. – Т. 60, No6. – С. 84–90.

Статья с описанием работы EfficientNet [электронный ресурс]. – Режим доступа: https://medium.com/@aniketthomas27/efficientnet-implementation-from-scratch-in-pytorch-a-step-by-step-guide-a7bb96f2bdaa (дата обращения: 25.12.2024).

Описание оптимизаторов [электронный ресурс]. – Режим доступа: https://proproprogs.ru/ml/ml-optimizatory-gradientnyh-algoritmov-rmsprop-adadelta-adam-nadam (дата обращения: 25.12.2024).

Stanford Cars Dataset [электронный ресурс]. – Режим доступа: http://ai.stanford.edu/~jkrause/cars/car_dataset.html (дата обращения: 25.12.2024).

Документация PyTorch [Электронный ресурс]. – Режим доступа: https://pytorch.org/docs/stable/ (дата обращения: 25.12.2024).